{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime as dt\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bigmart_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As there is some gap between 75% and max, there may be outliers.\n",
    "2. Since there is difference between mean and median, skewness appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. All the object type columns needs to be converted to integers or float. We can use LabelEncoders.\n",
    "2. There are some null values present in columns : Item_Weight and Outlet_Size. SimpleImputation can be used.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Univariate Analysis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types of fat content in items present in the store\n",
    "fat_count = df['Item_Fat_Content'].value_counts()\n",
    "fat_types = ['Low Fat','Regular','LF','reg','low fat']\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.barplot(fat_types,fat_count)\n",
    "plt.title(\"Item's Fat Content\")\n",
    "plt.xlabel(\"Fat Content\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Low fat and Regular are the two types of fat but they have spelled in various ways. Data Cleaning needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 items sold in the mart:\n",
    "item_type = df['Item_Type'].value_counts()[:10]\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.barplot(item_type.index,item_type.values,alpha=0.8)\n",
    "plt.title(\"Top 10 items sold in BigMart\")\n",
    "plt.xlabel(\"Item Types\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say from this graph that most people buy fruits and vegetables the most follwed by snacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BigMart Outlet Sizes\n",
    "outlet_size = df['Outlet_Size'].value_counts()\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(outlet_size.index,outlet_size.values,alpha=0.8)\n",
    "plt.title(\"BigMart Outlet Sizes\",fontsize=13)\n",
    "plt.xlabel(\"Outlet Size\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Item_MRP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know from this plot that mrp of the products ranges between 0 to 300 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Item_Outlet_Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a right skewed data and we can see that the sale price goes beyond 8k, which is a huge amount when compared to mrp."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bivariate Analysis : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null values :\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are null values in Item_Weight and Outlet_Size column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can drop Item_Identifier column \n",
    "\n",
    "df.drop('Item_Identifier',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Missing values : \n",
    "def fill_na(df):\n",
    "    mode = list(df['Outlet_Size'].mode())\n",
    "    #print(\"Mode : \", mode)\n",
    "    df['Outlet_Size'] = df['Outlet_Size'].fillna(mode[0])\n",
    "\n",
    "    mean = round(df['Item_Weight'].mean(),2)\n",
    "    #print(\"Mean : \", mean)\n",
    "    df['Item_Weight'] = df['Item_Weight'].fillna(mean)\n",
    "    return df\n",
    "\n",
    "df = fill_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates from Item_Fat_Content\n",
    "\n",
    "def remove_duplicates(df): \n",
    "    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace('reg','Regular')\n",
    "    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace('LF','Low Fat')\n",
    "    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace('low fat','Low Fat')\n",
    "    return df\n",
    "\n",
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(df):\n",
    "    df_new = df.copy()\n",
    "    df_new.drop('Item_Weight',axis=1,inplace=True)\n",
    "    df_new.drop('Item_Visibility',axis=1,inplace=True)\n",
    "    df_new.drop('Item_MRP',axis=1,inplace=True)\n",
    "    df_new.drop('Outlet_Establishment_Year',axis=1,inplace=True)\n",
    "    df_new.drop('Item_Outlet_Sales',axis=1,inplace=True)\n",
    "\n",
    "    col = df_new.columns.values\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    for each in range(len(col)):\n",
    "        le.fit(df[col[each]])\n",
    "        df[col[each]] = le.transform(df[col[each]])\n",
    "    df.head()    \n",
    "    \n",
    "    return df\n",
    "\n",
    "df = label_encode(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between columns :\n",
    "\n",
    "df_corr = df.corr()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_corr,annot=True)\n",
    "plt.title(\"Co-relation between the columns\",fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see from the heatmap that there is a positive correlation with the target variable 'Item_Outlet_Sales' in almost all columns except 'Item_Visibility','Outlet_Establishment_Year' and 'Outlet_Size'.\n",
    "2. Outlet_MRP is highly correlated to our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for skewness : \n",
    "\n",
    "print(\"Skewness value in all columns : \")\n",
    "print(df.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see  skewness in columns : ['Item_Fat_Content','Item_Visibility','Outlet_Type','Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness graph :\n",
    "col = df.columns.values\n",
    "plt.figure(figsize=(17,70))\n",
    "for i in range(0,len(col)):\n",
    "    plt.subplot(16,11,i+1)\n",
    "    sns.distplot(df[col[i]])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer('yeo-johnson')\n",
    "df = pd.DataFrame(pt.fit_transform(df))\n",
    "\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the skewness has been removed now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df.columns.values\n",
    "plt.figure(figsize=(15,100))\n",
    "for i in range(0,len(col)):\n",
    "    plt.subplot(16,11,i+1)\n",
    "    sns.boxplot(df[col[i]],palette='viridis',orient='v')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few outliers are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairplot\n",
    "plt.figure(figsize=(16,16))\n",
    "sns.pairplot(df)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "z =np.abs(zscore(df))\n",
    "print(np.where(z>3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[(z < 3).all(axis=1)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "def scale(df_new):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_new)\n",
    "    df_new = pd.DataFrame(scaler.transform(df_new))\n",
    "    return df_new\n",
    "df_new = scale(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_new.iloc[:,:10]\n",
    "y= df_new.iloc[:,10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_state_counter(model):\n",
    "    max_r2_score=0\n",
    "    for j in range(10,90):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=.20,random_state=j)\n",
    "        reg = model\n",
    "        reg.fit(x_train,y_train)\n",
    "        y_pred = reg.predict(x_test)\n",
    "        score = r2_score(y_test,y_pred)\n",
    "        if score>max_r2_score:\n",
    "            max_r2_score = score\n",
    "            final_state = j\n",
    "    return final_state,max_r2_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "svr = SVR()\n",
    "knr = KNR()\n",
    "dtr = DecisionTreeRegressor()\n",
    "rfr = RandomForestRegressor()\n",
    "model = [lr,svr,knr,dtr,rfr]\n",
    "\n",
    "for each in range(len(model)):\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(model[each])\n",
    "    seed,score = random_state_counter(model[each])\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Random State is \", seed , \"with score : \",score)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVR gives highest R2 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR : \n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=85)\n",
    "final_reg = svr\n",
    "final_reg.fit(x_train,y_train)\n",
    "y_pred = final_reg.predict(x_test)\n",
    "\n",
    "print(\"R2 Score : \",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Boost Regressor works best of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting : \n",
    "#Adaboost Regressor and RandomforestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "ada = AdaBoostRegressor(n_estimators=100,random_state=124)\n",
    "reg = GradientBoostingRegressor(n_estimators=100,random_state=13)\n",
    "\n",
    "boosting_model = [ada,reg]\n",
    "\n",
    "for i in range(len(boosting_model)):\n",
    "    boost = boosting_model[i]\n",
    "    boost.fit(x_train,y_train)\n",
    "    pred = boost.predict(x_test)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(boost)\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"R2 Score : \", r2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter distirbution\n",
    "param_dist = { \n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01,0.05,0.1,0.3,1],\n",
    "    'loss': ['linear','square','exponential']\n",
    "}\n",
    "\n",
    "\n",
    "jobs=-1\n",
    "ada_grid = GridSearchCV(estimator=ada,\n",
    "                      param_grid=param_dist,\n",
    "                      scoring='r2',\n",
    "                      cv=5,\n",
    "                      n_jobs=jobs)\n",
    "\n",
    "cv_score = cross_val_score(ada_grid,X,y,cv=5,scoring='r2')\n",
    "print(\"Cross Validation Score : \", cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error : \",mean_absolute_error(y_test,y_pred))\n",
    "print(\"Mean Squared Error  : \",mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(ada,'adamodel.obj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"bigmart_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = fill_na(testing_data)\n",
    "testing_data = remove_duplicates(testing_data)\n",
    "#testing_data.drop('Item_Identifier',axis=1,inplace=True)\n",
    "#testing_data = label_encode(testing_data)\n",
    "\n",
    "col = ['Item_Fat_Content','Item_Type','Outlet_Identifier','Outlet_Size','Outlet_Location_Type','Outlet_Type']\n",
    "le = LabelEncoder()\n",
    "for each in range(len(col)):\n",
    "        le.fit(testing_data[col[each]])\n",
    "        testing_data[col[each]] = le.transform(testing_data[col[each]])\n",
    "    \n",
    "\n",
    "testing_data = scale(testing_data)\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_from_joblib = joblib.load('adamodel.obj')\n",
    "svr_from_joblib.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
